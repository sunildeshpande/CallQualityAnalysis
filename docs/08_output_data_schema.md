# Output Data Schema - Sentiment Analysis Result

## Output JSON Structure

The output JSON for processed conversations follows this structure:

```json
{
  "report_metadata": {
    "report_id": "REPORT_20250122_001",
    "generated_at": "2025-01-22T15:45:00Z",
    "campaign_id": "CAMPAIGN_OFFERS_Q1",
    "transcription_count": 1,
    "analysis_duration_ms": 2340
  },
  "call_details": {
    "call_id": "CALL_20250122_001",
    "duration": "7m 2s",
    "sentiment_trend": "positive_increasing",
    "bot_performance_score": 8.2,
    "customer_engagement_score": 8.5
  },
  "quality_analysis": {
    "metrics": [
      {
        "question_id": "Q001",
        "question": "Was the call focused on the campaign topic?",
        "answer": "Yes",
        "confidence": 0.94,
        "supporting_evidence": [
          "Bot mentioned credit card benefits (timestamp: 0-120s)",
          "Discussion focused on offer details for 6+ minutes",
          "Keywords matched: 72% of campaign keyword list"
        ],
        "answer_type": "boolean"
      },
      {
        "question_id": "Q002",
        "question": "What is the overall sentiment of the customer?",
        "answer": "Positive",
        "sentiment_score": 0.78,
        "sentiment_trajectory": [
          {"segment": "Opening", "sentiment": "neutral"},
          {"segment": "Offer presentation", "sentiment": "positive"},
          {"segment": "Q&A", "sentiment": "positive"},
          {"segment": "Closing", "sentiment": "positive"}
        ],
        "answer_type": "categorical"
      },
      {
        "question_id": "Q003",
        "question": "Did the customer raise new requests or queries?",
        "answer": "Yes",
        "new_requests": [
          "Annual fee waiver eligibility",
          "Insurance coverage details",
          "APR comparison with existing card"
        ],
        "unresolved_count": 1,
        "answer_type": "boolean_with_details"
      },
      {
        "question_id": "Q004",
        "question": "What percentage of call time was spent on the main offer?",
        "answer": "72%",
        "time_breakdown": {
          "main_offer": "5m 2s",
          "additional_queries": "1m 23s",
          "closing": "0m 37s",
          "total": "7m 2s"
        },
        "answer_type": "numeric"
      },
      {
        "question_id": "Q005",
        "question": "Was the customer interested in proceeding?",
        "answer": "Interested",
        "interest_level": 0.82,
        "next_steps": "Customer requested product comparison; likely to convert",
        "conversion_probability": 0.75,
        "answer_type": "categorical"
      }
    ]
  },
  "summary_insights": {
    "overall_quality_score": 8.35,
    "strengths": [
      "Excellent topic alignment with campaign",
      "Strong customer engagement and positive sentiment",
      "Comprehensive answer to customer questions"
    ],
    "areas_for_improvement": [
      "Could have proactively addressed APR comparison",
      "One customer query remains unresolved"
    ],
    "recommendations": [
      "Escalate for product comparison follow-up",
      "Add APR comparison to bot's knowledge base"
    ]
  }
}
```

## Field Specifications

### Report Metadata
- **report_id** (string, required): Unique identifier for the report
  - Format: `REPORT_{date}_{sequence}` or UUID
  - Generated by the system
  
- **generated_at** (string, required): ISO8601 datetime when report was generated
  - Format: `YYYY-MM-DDTHH:mm:ssZ`
  
- **campaign_id** (string, required): Campaign identifier from input
  - Matches the campaign_id from the input transcription
  
- **transcription_count** (integer, required): Number of transcriptions processed in this report
  - Typically 1 for single transcription, but can be >1 for batch processing
  
- **analysis_duration_ms** (integer, required): Time taken to process in milliseconds
  - Processing time for sentiment analysis and quality metrics

### Call Details
- **call_id** (string, required): Call identifier from input
  - Matches the call_id from the input transcription
  
- **duration** (string, required): Human-readable call duration
  - Format: `{minutes}m {seconds}s` or similar
  - Derived from `duration_seconds` in input
  
- **sentiment_trend** (string, required): Overall sentiment trend
  - Possible values: `positive_increasing`, `positive_decreasing`, `negative_increasing`, `negative_decreasing`, `neutral`, `mixed`
  - To be confirmed: exact list of valid values
  
- **bot_performance_score** (float, required): Score for bot performance
  - Range: 0.0 to 10.0 (to be confirmed)
  - Calculated based on various metrics
  
- **customer_engagement_score** (float, required): Score for customer engagement
  - Range: 0.0 to 10.0 (to be confirmed)
  - Measures customer participation and interest

### Quality Analysis
- **metrics** (array, required): Array of quality metric objects
  - Contains predefined questions and their answers
  - Each metric represents a quality assessment question

#### Metric Object
- **question_id** (string, required): Unique identifier for the question
  - Format: `Q{number}` (e.g., "Q001", "Q002")
  - Predefined question identifiers
  
- **question** (string, required): The quality assessment question
  - Human-readable question text
  
- **answer** (string/number, required): The answer to the question
  - Type depends on `answer_type`
  - Can be: "Yes"/"No", "Positive"/"Negative", percentage string, etc.
  
- **answer_type** (string, required): Type of answer
  - Possible values: `boolean`, `categorical`, `numeric`, `boolean_with_details`
  - Determines which additional fields are present
  
- **confidence** (float, optional): Confidence score for the answer
  - Range: 0.0 to 1.0
  - Present for some answer types
  
- **supporting_evidence** (array, optional): Evidence supporting the answer
  - Array of strings describing evidence
  - Present for some answer types
  
- **sentiment_score** (float, optional): Sentiment score
  - Range: -1.0 to 1.0 or 0.0 to 1.0 (to be confirmed)
  - Present for sentiment-related questions
  
- **sentiment_trajectory** (array, optional): Sentiment over time segments
  - Array of objects with `segment` and `sentiment` fields
  - Present for sentiment-related questions
  
- **new_requests** (array, optional): List of new requests/queries
  - Array of strings
  - Present for questions about customer requests
  
- **unresolved_count** (integer, optional): Count of unresolved items
  - Present for questions about requests/queries
  
- **time_breakdown** (object, optional): Time breakdown by category
  - Object with time strings for different categories
  - Present for time-related questions
  
- **interest_level** (float, optional): Interest level score
  - Range: 0.0 to 1.0
  - Present for interest-related questions
  
- **next_steps** (string, optional): Description of next steps
  - Human-readable text
  - Present for interest/conversion questions
  
- **conversion_probability** (float, optional): Probability of conversion
  - Range: 0.0 to 1.0
  - Present for conversion-related questions

### Summary Insights
- **overall_quality_score** (float, required): Overall quality score
  - Range: 0.0 to 10.0 (to be confirmed)
  - Aggregated score from all metrics
  
- **strengths** (array, required): List of identified strengths
  - Array of strings
  - Can be empty array
  
- **areas_for_improvement** (array, required): List of areas needing improvement
  - Array of strings
  - Can be empty array
  
- **recommendations** (array, required): Actionable recommendations
  - Array of strings
  - Can be empty array

## Questions to Clarify

1. **Question Set**: Are the 5 questions (Q001-Q005) fixed, or can they vary by campaign?
2. **Answer Types**: Are there other answer types beyond the 4 shown?
3. **Score Ranges**: What are the exact ranges for scores (0-10, 0-1, etc.)?
4. **Sentiment Values**: What are all valid values for `sentiment_trend` and segment `sentiment`?
5. **Duration Format**: Is the duration format always "Xm Ys" or can it vary?
6. **Batch Processing**: For multiple transcriptions, is there one report per transcription or one combined report?
7. **Metric Customization**: Can metrics/questions be customized per campaign or tenant?
8. **Stub Implementation**: For the stub, should we return realistic mock data or simple placeholder values?

## Stub Implementation Notes

For the initial stub implementation, we should:
- Generate realistic-looking mock data
- Use the exact structure shown above
- Ensure all required fields are present
- Use appropriate data types and formats
- Can use random but reasonable values for scores and metrics

